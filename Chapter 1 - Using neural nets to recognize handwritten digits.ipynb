{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Perceptrons and sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let $P_0$ be a perceptron, and $P_1$ be the perceptron obtained by multiplying $P_0$'s weights and biases by a constant $c>0$. For any neuron $N_0$ in $P_0$ with weights $(w_1, ..., w_n)$ and bias $b$, the weights and bias of the corresponding neuron $N_1$ in $P_1$ are respectively $(c w_1, ..., c w_n)$ and $c b$.\n",
    "\n",
    "Given an input vector $(x_1, ..., x_n)$, $N_0$ fires if and only if $\\sum_j w_j x_j + b > 0$. This condition is equivalent to $c (\\sum_j w_j x_j + b) > 0$, i.e. to the firing of $N_1$. By induction on the number of layers, $P_0$ and $P_1$'s outputs are therefore identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, let $P_0$ be the original network, and $P_1$ be its modification. Let $N_0$ be a neuron in $P_0$ with weights $(w_1, ..., w_n)$ and bias $b$. \n",
    "* When fed with input $(x_1^c, ..., x_n^c)$, $N_0$ returns:\n",
    "$$output_0^c = \\mathbb{1}_{\\mathbb{R}_+^*}(\\sum_j w_j x_j^c + b)$$\n",
    "\n",
    "* The corresponding neuron $N_1$ in $P_1$ returns:\n",
    "$$output_1^c = \\frac {1} {1 + exp[-c(\\sum_j w_j x_j^c + b)]}$$\n",
    "\n",
    "Suppose then that as $c \\to \\infty$, we have $(x_1^c, ..., x_n^c) \\to (x_1^{\\infty}, ..., x_n^{\\infty})$, with $\\sum_j w_j x_j^{\\infty} + b \\neq 0$. Then:\n",
    "\n",
    "* $output_0^c \\to \\mathbb{1}_{\\mathbb{R}_+^*}(\\sum_j w_j x_j^{\\infty} + b)$\n",
    "\n",
    "* $output_1^c \\to output_1^{\\infty} = \\lim\\limits_{c \\rightarrow +\\infty}  \\frac {1} {1 + exp[-c(\\sum_j w_j x_j^c + b)]} = \\mathbb{1}_{\\mathbb{R}_+^*}(\\sum_j w_j x_j^{\\infty} + b)$\n",
    "\n",
    "Therefore $\\lim\\limits_{c \\rightarrow +\\infty} output_0^c = \\lim\\limits_{c \\rightarrow +\\infty} output_1^c$. As $c \\to \\infty$ the behaviour of $N_1$ is exactly the same as that of $N_0$. This enables a *rigourous* proof (again, by induction on the layer) that both *networks* behave the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the case of an *input layer* neuron fed with $(x_1, ..., x_n)$, such that  $\\sum_j w_j x_j + b = 0$ (here, the input does *not* depend on c). Then $output_0^c = 0$ whereas $output_1^c = 0.5$. Both neurons behave very differently, and so will the networks they are a part of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Learning with gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: how Gradient Descent is optimal-ish.\n",
    "* Suppose we're at some point $x \\in \\mathbb{R}^n$, and we're looking for a new point $x + \\Delta v$ in $x$'s neighbourhood that will hopefully get us closer to the minimum. To achieve this, a good heuristic is to make $C(x + \\Delta v)$ as small as possible. Since $C(x + \\Delta v) \\simeq C(x) + \\nabla C \\cdot \\Delta v$, this is equivalent to minimizing the function $$f : \\Delta v \\in \\mathbb{R}^n \\mapsto  \\nabla C \\cdot \\Delta v$$ subject to constraint: $||\\Delta v|| = \\epsilon$.\n",
    "\n",
    "\n",
    "\n",
    "* Cauchy-Schwarz shows that: $| f(\\Delta v) | = |\\nabla C \\cdot \\Delta | \\leq ||\\nabla C|| \\times ||\\Delta v||$. Thus $f(\\Delta v) \\geq - \\epsilon ||\\nabla C||$. Moreover, taking $\\eta = \\frac {\\epsilon} {||\\nabla C||}$ yields $f(- \\eta \\nabla C) = - \\epsilon ||\\nabla C||$, with $||- \\eta \\nabla C|| = \\epsilon$.\n",
    "\n",
    "\n",
    "\n",
    "* Therefore taking $\\Delta v = - \\eta \\nabla C$ solves the optimization problem. Gradient descent is **locally** optimal: it makes the greedy choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: let's code Gradient Descent in 1D !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function finds the minimum of a smooth function $f: \\mathbb{R}  \\to \\mathbb{R}$ using gradient descent with a fixed learning rate. It also plots both the function and the steps it makes until the minimum. It takes as arguments:\n",
    "* the function itself, $f$\n",
    "* $f' = \\nabla f$, the derivative of $f$ (computing derivatives, symbolically or numerically, is possible but tedious)\n",
    "* the learning rate $\\eta$\n",
    "* the stopping parameter, $\\epsilon$: the function stops when $|| \\nabla f || \\leq \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.0036561584400629744"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GD_minimum(f, f_prime, x_0, eta, epsilon):\n",
    "    x_func = np.linspace(-np.abs(x_0), np.abs(x_0), 1000)\n",
    "    y_func = f(x_func)\n",
    "    \n",
    "    x_values = [x_0]\n",
    "    y_values = [f(x_0)]\n",
    "    \n",
    "    x_temp = x_0\n",
    "    \n",
    "    while np.abs(f_prime(x_temp)) > epsilon:\n",
    "        x_temp -= eta*f_prime(x_temp)\n",
    "        x_values.append(x_temp)\n",
    "        y_values.append(f(x_temp))\n",
    "    \n",
    "    plt.plot(x_func, y_func)\n",
    "    plt.plot(x_values, y_values)\n",
    "    plt.show()\n",
    "    \n",
    "    return x_temp\n",
    "\n",
    "def f_0(x):\n",
    "    return x*x\n",
    "\n",
    "def f_0_prime(x):\n",
    "    return 2*x\n",
    "\n",
    "GD_minimum(f_0, f_0_prime, -100, 0.2, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the algorithm stops at a point $x^*$ when $\\nabla f(x^*) \\simeq 0$. Yet we know that even $\\nabla f(x^*) = 0$ is not a sufficient condition to ensure $x^*$'s optimality: picture a saddle. One important situation when this is enough is when $f$ is convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Stochastic Gradient Descent and Online learning\n",
    "\n",
    "**Stochastic gradient descent** is a computationally inexpensive variant of gradient descent for neural networks. Remember, the formula for the cost function: $C(w, b) = \\frac {1} {2n} \\sum \\limits_{x \\in X} ||y(x)-a||^2 $ with $X$ the training set. \n",
    "\n",
    "Taking $C_x := ||y(x)-a||^2$, we can rewrite $C(w, b) = \\frac {1} {2n} \\sum \\limits_{x \\in X} C_x $. Computing $\\nabla C(w, b) = \\frac {1} {n} \\sum \\limits_{x \\in X} \\nabla C_x $ would involve a gradient calculation for every training input. Instead, SGD chooses a **mini-batch** $\\{ X_1, ..., X_m \\} \\subset X$ of size $m << n$ at **random**, and computes $\\frac {1} {m} \\sum \\limits_{1 \\leq i \\leq m} \\nabla C_{X_i} \\simeq \\frac {1} {n} \\sum \\limits_{x \\in X} \\nabla C_x$ to adjust the weights and biases.\n",
    "\n",
    "**Online learning** is SGD with mini-batches of size $1$. This is especially useful when training inputs are being added during learning. However, there is no guarantee that each step will approximate the gradient correctly, whereas results such as the *central limit theorem* show that $\\frac {1} {m} \\sum \\limits_{1 \\leq i \\leq m} \\nabla C_{X_i}$ is close enough to the actual gradient for reasonably big values of $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Implementing our network to classify digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy pasted some of Mike's code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the data\n",
    "This is taken from ``mnist_loader.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('NNDL_file/data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Neural Network\n",
    "\n",
    "We start with a ``Network`` class:\n",
    "* ``sizes`` is a list containing the number of neurons in each layer\n",
    "* biases and weights are initialized randomly, from normal distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance ``net`` of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.66141604  0.35533732]\n",
      " [ 0.32933628  1.39098231]\n",
      " [ 0.69897283  0.25768621]]\n",
      "\n",
      "[[-0.54250587  0.77577761  3.00767679]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 2.08337569],\n",
       "        [-1.4889273 ],\n",
       "        [ 0.01552329]]), array([[0.18657768]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([2, 3, 1])\n",
    "\n",
    "def M(i):\n",
    "    return net.weights[i]\n",
    "\n",
    "print(M(0))\n",
    "print()\n",
    "print(M(1))\n",
    "\n",
    "net.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M(i)$ is a numpy matrix storing the weights connecting neuron layers $i+1$ and $i+2$. To be more precise, $[M(i)]_{jk}$ is the weight for the connection between the $k^{th}$ neuron in the second layer, and the $j^{th}$ neuron in the third layer. This notation is counterintuitive: at first glance, using $M(i)$'s transpose would be easier. However, using $M(i)$ will simplify the algebra later.\n",
    "\n",
    "The vector output of all the neurons in layer $i+2$ is given by:\n",
    "\n",
    "$$ output_{i+2} = \\sigma (M(i) \\times output_{i+1} + b_{i+2}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\             (update)$$\n",
    "\n",
    "The proof for this is easy, it's just important not to get too confused with the notation, which is further complicated by the Python conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing the output of a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the sigmoid function, which is vectorized automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add a method to the Network class, called ``feedforward``:\n",
    "* argument: input $a$ for the network, of type ``(n, 1) Numpy ndarray``\n",
    "* returns: the output of the network if $a$ is input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "    \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the Stochastic Gradient Descent itself. The code works as follows:\n",
    "\n",
    "In each epoch, the algorithm has to build mini-batches:\n",
    "* it starts by randomly shuffling the training data \n",
    "* it then partitions it into mini-batches of the appropriate size \n",
    "\n",
    "Then, the program has to modify the weights and biases:\n",
    "* for each mini-batch, ``self.update_mini_batch(mini_batch, eta)`` updates the network weights and biases according to a single iteration of gradient descent using just the training data in ``mini_batch``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If \"test_data\" is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait so ``self.update_mini_batch(mini_batch, eta)`` is doing all the work here right?\"Yes. Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait so it's ``self.backprop(x, y)`` that's doing all the work here in fact?\" Yes again. Backpropagation is an algorithm that computes the gradient of the cost function. We'll see how it works in more detail in the next chapter. For now we'll assume we know that it works, and use it with a clear conscience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A few additional functions so we can test \n",
    "\n",
    "\n",
    "So far we've coded the following classes and functions:\n",
    "* ``Network`` class\n",
    "* ``feedforward`` method\n",
    "* ``update_mini_batch`` method\n",
    "* ``SGD`` method\n",
    "* ``sigmoid`` function\n",
    "\n",
    "We need a few additional functions:\n",
    "* the ``backprop`` method, as mentioned above\n",
    "* the ``cost_derivative`` method, that is used in backprop\n",
    "* ``sigmoid_prime``, the derivative of the sigmoid function\n",
    "* an ``evaluate`` method that returns the number of inputs for which the network got it right\n",
    "\n",
    "\n",
    "Below is the code for all these guys: we're not interested in them right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If \"test_data\" is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the whole thing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "test_data = list(test_data)\n",
    "validation_data = list(validation_data)\n",
    "training_data = list(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST inputs are $28 \\times 28$ images, therefore the **input layer** is made up of $28 \\times 28 = 784$ input neurons.\n",
    "The output is an individual digit: as above, the **output layer** therefore contains $9$ neurons.\n",
    "We will here be using one **hidden layer** containing $30$ neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Stochastic Gradient Descent on our network, with:\n",
    "* $30$ epochs\n",
    "* mini-batches of size $10$\n",
    "* a learning rate $\\eta = 3.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9044 / 10000\n",
      "Epoch 1: 9199 / 10000\n",
      "Epoch 2: 9324 / 10000\n",
      "Epoch 3: 9333 / 10000\n",
      "Epoch 4: 9319 / 10000\n",
      "Epoch 5: 9376 / 10000\n",
      "Epoch 6: 9364 / 10000\n",
      "Epoch 7: 9355 / 10000\n",
      "Epoch 8: 9406 / 10000\n",
      "Epoch 9: 9439 / 10000\n",
      "Epoch 10: 9372 / 10000\n",
      "Epoch 11: 9450 / 10000\n",
      "Epoch 12: 9424 / 10000\n",
      "Epoch 13: 9463 / 10000\n",
      "Epoch 14: 9433 / 10000\n",
      "Epoch 15: 9430 / 10000\n",
      "Epoch 16: 9447 / 10000\n",
      "Epoch 17: 9446 / 10000\n",
      "Epoch 18: 9443 / 10000\n",
      "Epoch 19: 9448 / 10000\n",
      "Epoch 20: 9421 / 10000\n",
      "Epoch 21: 9456 / 10000\n",
      "Epoch 22: 9457 / 10000\n",
      "Epoch 23: 9456 / 10000\n",
      "Epoch 24: 9446 / 10000\n",
      "Epoch 25: 9455 / 10000\n",
      "Epoch 26: 9468 / 10000\n",
      "Epoch 27: 9440 / 10000\n",
      "Epoch 28: 9474 / 10000\n",
      "Epoch 29: 9459 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Very) Minor differences with Mike's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This runs on Python 3 so\n",
    "    - Because zip doesn't return a list anymore, I converted training, test and validation data into lists (https://stackoverflow.com/questions/27431390/typeerror-zip-object-is-not-subscriptable) \n",
    "    - I used ``range`` instead of ``xrange``\n",
    "    - I replaced ``cPickle`` with ``pickle``, and ``pickle.load(f)`` \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise: a network without hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_two_layers = Network([784, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Stochastic Gradient Descent on this network too, with, as above:\n",
    "* $30$ epochs\n",
    "* mini-batches of size $10$\n",
    "* a learning rate $\\eta = 3.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7203 / 10000\n",
      "Epoch 1: 7382 / 10000\n",
      "Epoch 2: 7441 / 10000\n",
      "Epoch 3: 7468 / 10000\n",
      "Epoch 4: 7438 / 10000\n",
      "Epoch 5: 7430 / 10000\n",
      "Epoch 6: 7463 / 10000\n",
      "Epoch 7: 7480 / 10000\n",
      "Epoch 8: 7483 / 10000\n",
      "Epoch 9: 7500 / 10000\n",
      "Epoch 10: 7501 / 10000\n",
      "Epoch 11: 7498 / 10000\n",
      "Epoch 12: 8307 / 10000\n",
      "Epoch 13: 8284 / 10000\n",
      "Epoch 14: 8317 / 10000\n",
      "Epoch 15: 8333 / 10000\n",
      "Epoch 16: 8362 / 10000\n",
      "Epoch 17: 8356 / 10000\n",
      "Epoch 18: 8361 / 10000\n",
      "Epoch 19: 8339 / 10000\n",
      "Epoch 20: 8347 / 10000\n",
      "Epoch 21: 8316 / 10000\n",
      "Epoch 22: 8334 / 10000\n",
      "Epoch 23: 8333 / 10000\n",
      "Epoch 24: 8354 / 10000\n",
      "Epoch 25: 8342 / 10000\n",
      "Epoch 26: 8345 / 10000\n",
      "Epoch 27: 8354 / 10000\n",
      "Epoch 28: 8348 / 10000\n",
      "Epoch 29: 8354 / 10000\n"
     ]
    }
   ],
   "source": [
    "net_two_layers.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not as good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
