{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. $L^2$ regularization\n",
    "\n",
    "As we saw previously, overfitting is what happens when variance is too big. Regularization is a way to solve this issue without impeding the network's power too much: the idea is to penalize model complexity by modifying the cost function.\n",
    "\n",
    "\n",
    "We therefore introduce the following cost:\n",
    "$$C = C_0 + \\frac {\\lambda} {2n} \\sum_w w^2$$\n",
    "where $\\lambda \\in \\mathbb{R}+$ is called the **regularization parameter**, and $C_0$ is a non-regularized cost function. For instance if $C_0$ is the cross entropy cost function:\n",
    "$$C = - \\frac {1} {n} \\sum_x yln(a) + (1-y)ln(1-a) + \\frac {\\lambda} {2n} \\sum_w w^2$$\n",
    "\n",
    "And when $C_0$ is the quadratic cost function:\n",
    "$$C = - \\frac {1} {n} \\sum_x ||y(x)-a||^2 + \\frac {\\lambda} {2n} \\sum_w w^2$$\n",
    "\n",
    "$\\lambda$ represents whether we are more concerned with bias or variance. Notice that we haven't yet shown how exactly penalizing big weights is tantamount to avoiding excessive model complexity.\n",
    "\n",
    "Regularization doesn't make gradient descent much more complicated: the rate of change of $C_0$ with respect to the weights and biases can be computed using backpropagation, and the gradient is given by $\\frac {\\partial C} {\\partial w} = \\frac {\\partial C_0} {\\partial w} + \\frac {\\lambda w} {n}$ and $\\frac {\\partial C} {\\partial b} = \\frac {\\partial C_0} {\\partial b}$. Stochastic gradient descent also happens naturally (for weights, approximate the gradient of $C_0$ and then add the $\\frac {\\lambda w} {n}$ term, for biases, regularization doen't change anything).\n",
    "\n",
    "### Testing $L^2$ regularization\n",
    "Let's test these changes on the neural network that we used in the previous part to showcase the ills of overfitting. As often, this code is Michael Nielsen's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what's in ``mnist_loader.py``, *i.e.* the functions that load MNIST the right way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('NNDL_file/data/mnist.pkl.gz')\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again to import the data I had to tinker a little bit with the code so that it would work in Python 3:\n",
    "* changed ``cPickle`` into ``Pickle``\n",
    "* converted ``test_data``, ``validation_data``, and ``training_data`` into lists for them to be iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1.284895665521856\n",
      "Accuracy on training data: 4197 / 5000\n",
      "Cost on evaluation data: 1.3106159930648673\n",
      "Accuracy on evaluation data: 8079 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.0055176989939365\n",
      "Accuracy on training data: 4419 / 5000\n",
      "Cost on evaluation data: 1.0855912301437893\n",
      "Accuracy on evaluation data: 8466 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.8536924837589502\n",
      "Accuracy on training data: 4548 / 5000\n",
      "Cost on evaluation data: 0.9843839284571787\n",
      "Accuracy on evaluation data: 8637 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.7550429847899467\n",
      "Accuracy on training data: 4621 / 5000\n",
      "Cost on evaluation data: 0.9094653055005214\n",
      "Accuracy on evaluation data: 8759 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.6960369940641402\n",
      "Accuracy on training data: 4665 / 5000\n",
      "Cost on evaluation data: 0.8945695614612817\n",
      "Accuracy on evaluation data: 8771 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.6381167275799088\n",
      "Accuracy on training data: 4735 / 5000\n",
      "Cost on evaluation data: 0.8415109596027928\n",
      "Accuracy on evaluation data: 8859 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.5773369380454864\n",
      "Accuracy on training data: 4786 / 5000\n",
      "Cost on evaluation data: 0.8204983364947116\n",
      "Accuracy on evaluation data: 8923 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.5518542263044559\n",
      "Accuracy on training data: 4804 / 5000\n",
      "Cost on evaluation data: 0.8090360262976025\n",
      "Accuracy on evaluation data: 8935 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.5269333373923851\n",
      "Accuracy on training data: 4822 / 5000\n",
      "Cost on evaluation data: 0.8116160868923874\n",
      "Accuracy on evaluation data: 8934 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.49692560760613136\n",
      "Accuracy on training data: 4838 / 5000\n",
      "Cost on evaluation data: 0.8034750226751799\n",
      "Accuracy on evaluation data: 8956 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.48830731774508096\n",
      "Accuracy on training data: 4846 / 5000\n",
      "Cost on evaluation data: 0.7942653705778933\n",
      "Accuracy on evaluation data: 8985 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.4747061821230185\n",
      "Accuracy on training data: 4855 / 5000\n",
      "Cost on evaluation data: 0.8038803916433308\n",
      "Accuracy on evaluation data: 8992 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.4300847789579307\n",
      "Accuracy on training data: 4883 / 5000\n",
      "Cost on evaluation data: 0.7770778882762346\n",
      "Accuracy on evaluation data: 8993 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.4320789613225059\n",
      "Accuracy on training data: 4877 / 5000\n",
      "Cost on evaluation data: 0.7981207997530511\n",
      "Accuracy on evaluation data: 8990 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.4218290287068238\n",
      "Accuracy on training data: 4889 / 5000\n",
      "Cost on evaluation data: 0.7956941230539883\n",
      "Accuracy on evaluation data: 8982 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.39554494682486696\n",
      "Accuracy on training data: 4915 / 5000\n",
      "Cost on evaluation data: 0.7940044854286028\n",
      "Accuracy on evaluation data: 9016 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.3814449259649054\n",
      "Accuracy on training data: 4929 / 5000\n",
      "Cost on evaluation data: 0.7884098905590371\n",
      "Accuracy on evaluation data: 9003 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.37409461543905426\n",
      "Accuracy on training data: 4925 / 5000\n",
      "Cost on evaluation data: 0.7877065958214239\n",
      "Accuracy on evaluation data: 9026 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.3661659404661851\n",
      "Accuracy on training data: 4934 / 5000\n",
      "Cost on evaluation data: 0.8017391447527585\n",
      "Accuracy on evaluation data: 9017 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.3571608166631301\n",
      "Accuracy on training data: 4927 / 5000\n",
      "Cost on evaluation data: 0.8027758975024939\n",
      "Accuracy on evaluation data: 9020 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.35442988574061696\n",
      "Accuracy on training data: 4941 / 5000\n",
      "Cost on evaluation data: 0.7988330503141711\n",
      "Accuracy on evaluation data: 9006 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.34033121288269363\n",
      "Accuracy on training data: 4944 / 5000\n",
      "Cost on evaluation data: 0.7863208848437392\n",
      "Accuracy on evaluation data: 9031 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.3404149388157005\n",
      "Accuracy on training data: 4945 / 5000\n",
      "Cost on evaluation data: 0.8062766861606575\n",
      "Accuracy on evaluation data: 9028 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.33691583855508334\n",
      "Accuracy on training data: 4950 / 5000\n",
      "Cost on evaluation data: 0.8078216521889695\n",
      "Accuracy on evaluation data: 9039 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.31924086626382475\n",
      "Accuracy on training data: 4955 / 5000\n",
      "Cost on evaluation data: 0.7928126400822251\n",
      "Accuracy on evaluation data: 9042 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.3232712795012312\n",
      "Accuracy on training data: 4951 / 5000\n",
      "Cost on evaluation data: 0.8035341395445394\n",
      "Accuracy on evaluation data: 9057 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.32056215329090326\n",
      "Accuracy on training data: 4948 / 5000\n",
      "Cost on evaluation data: 0.8018307906259353\n",
      "Accuracy on evaluation data: 9038 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.30648091472431005\n",
      "Accuracy on training data: 4965 / 5000\n",
      "Cost on evaluation data: 0.8031647596743628\n",
      "Accuracy on evaluation data: 9058 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.2967405359204987\n",
      "Accuracy on training data: 4963 / 5000\n",
      "Cost on evaluation data: 0.7822509848715086\n",
      "Accuracy on evaluation data: 9082 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.29503943802803223\n",
      "Accuracy on training data: 4966 / 5000\n",
      "Cost on evaluation data: 0.7971092685492595\n",
      "Accuracy on evaluation data: 9063 / 10000\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.29213008487063724\n",
      "Accuracy on training data: 4970 / 5000\n",
      "Cost on evaluation data: 0.7907430401115221\n",
      "Accuracy on evaluation data: 9057 / 10000\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.2875801502380281\n",
      "Accuracy on training data: 4966 / 5000\n",
      "Cost on evaluation data: 0.7936968528534871\n",
      "Accuracy on evaluation data: 9053 / 10000\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.28656812403820026\n",
      "Accuracy on training data: 4967 / 5000\n",
      "Cost on evaluation data: 0.8043262167429458\n",
      "Accuracy on evaluation data: 9064 / 10000\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.2773597726670703\n",
      "Accuracy on training data: 4973 / 5000\n",
      "Cost on evaluation data: 0.7945733489275504\n",
      "Accuracy on evaluation data: 9065 / 10000\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.2750230398337855\n",
      "Accuracy on training data: 4973 / 5000\n",
      "Cost on evaluation data: 0.7980267946316202\n",
      "Accuracy on evaluation data: 9069 / 10000\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.2719899184710543\n",
      "Accuracy on training data: 4975 / 5000\n",
      "Cost on evaluation data: 0.794346412040628\n",
      "Accuracy on evaluation data: 9071 / 10000\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.2691230281521867\n",
      "Accuracy on training data: 4973 / 5000\n",
      "Cost on evaluation data: 0.8000039174875897\n",
      "Accuracy on evaluation data: 9067 / 10000\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.2663643738569248\n",
      "Accuracy on training data: 4975 / 5000\n",
      "Cost on evaluation data: 0.8080880198633662\n",
      "Accuracy on evaluation data: 9058 / 10000\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.2606307914361742\n",
      "Accuracy on training data: 4976 / 5000\n",
      "Cost on evaluation data: 0.7984337229858933\n",
      "Accuracy on evaluation data: 9076 / 10000\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.2577303229775922\n",
      "Accuracy on training data: 4977 / 5000\n",
      "Cost on evaluation data: 0.7959351427051463\n",
      "Accuracy on evaluation data: 9083 / 10000\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.2546552137479804\n",
      "Accuracy on training data: 4977 / 5000\n",
      "Cost on evaluation data: 0.8023341130435928\n",
      "Accuracy on evaluation data: 9074 / 10000\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.25152966927047427\n",
      "Accuracy on training data: 4978 / 5000\n",
      "Cost on evaluation data: 0.8010288177221987\n",
      "Accuracy on evaluation data: 9072 / 10000\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.2494053444198024\n",
      "Accuracy on training data: 4978 / 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 0.8019259984651419\n",
      "Accuracy on evaluation data: 9073 / 10000\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.24750876087574938\n",
      "Accuracy on training data: 4978 / 5000\n",
      "Cost on evaluation data: 0.8045629180223859\n",
      "Accuracy on evaluation data: 9059 / 10000\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.24400801383099724\n",
      "Accuracy on training data: 4980 / 5000\n",
      "Cost on evaluation data: 0.8033741505093722\n",
      "Accuracy on evaluation data: 9061 / 10000\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.24063322736834436\n",
      "Accuracy on training data: 4980 / 5000\n",
      "Cost on evaluation data: 0.8024106536035334\n",
      "Accuracy on evaluation data: 9062 / 10000\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.23730865314686692\n",
      "Accuracy on training data: 4985 / 5000\n",
      "Cost on evaluation data: 0.8021904936600364\n",
      "Accuracy on evaluation data: 9064 / 10000\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.2351534627384445\n",
      "Accuracy on training data: 4986 / 5000\n",
      "Cost on evaluation data: 0.8032642073510604\n",
      "Accuracy on evaluation data: 9077 / 10000\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.2343316244177092\n",
      "Accuracy on training data: 4984 / 5000\n",
      "Cost on evaluation data: 0.8046191778285002\n",
      "Accuracy on evaluation data: 9059 / 10000\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.23073728346985986\n",
      "Accuracy on training data: 4985 / 5000\n",
      "Cost on evaluation data: 0.8070050533654469\n",
      "Accuracy on evaluation data: 9068 / 10000\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.2280545958944444\n",
      "Accuracy on training data: 4985 / 5000\n",
      "Cost on evaluation data: 0.8122800394412028\n",
      "Accuracy on evaluation data: 9067 / 10000\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.22540541821570115\n",
      "Accuracy on training data: 4985 / 5000\n",
      "Cost on evaluation data: 0.8043140809991849\n",
      "Accuracy on evaluation data: 9065 / 10000\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.22280003375165358\n",
      "Accuracy on training data: 4987 / 5000\n",
      "Cost on evaluation data: 0.8018189786076194\n",
      "Accuracy on evaluation data: 9069 / 10000\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.22171997482629113\n",
      "Accuracy on training data: 4986 / 5000\n",
      "Cost on evaluation data: 0.8069692524874178\n",
      "Accuracy on evaluation data: 9079 / 10000\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.2182822118429678\n",
      "Accuracy on training data: 4986 / 5000\n",
      "Cost on evaluation data: 0.8058970639651274\n",
      "Accuracy on evaluation data: 9062 / 10000\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.21770652907544058\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.815121767091299\n",
      "Accuracy on evaluation data: 9072 / 10000\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.21499548439312924\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8002173162835469\n",
      "Accuracy on evaluation data: 9081 / 10000\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.2126100086564361\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8050256493357917\n",
      "Accuracy on evaluation data: 9087 / 10000\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.21043675859431993\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8037406868163166\n",
      "Accuracy on evaluation data: 9075 / 10000\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.2085881785205787\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8066608288823202\n",
      "Accuracy on evaluation data: 9083 / 10000\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.20667396906605495\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8063111483398842\n",
      "Accuracy on evaluation data: 9094 / 10000\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.2050870999781125\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8061608624360252\n",
      "Accuracy on evaluation data: 9083 / 10000\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.20377729929720967\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.7975745014823684\n",
      "Accuracy on evaluation data: 9093 / 10000\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.20109625078800397\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8001368349905268\n",
      "Accuracy on evaluation data: 9086 / 10000\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.2006278480087793\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.8075102725400093\n",
      "Accuracy on evaluation data: 9090 / 10000\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.19760283223756947\n",
      "Accuracy on training data: 4989 / 5000\n",
      "Cost on evaluation data: 0.8047378436694611\n",
      "Accuracy on evaluation data: 9085 / 10000\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.19605053493004432\n",
      "Accuracy on training data: 4989 / 5000\n",
      "Cost on evaluation data: 0.7988063590169261\n",
      "Accuracy on evaluation data: 9081 / 10000\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.19424954022558166\n",
      "Accuracy on training data: 4989 / 5000\n",
      "Cost on evaluation data: 0.7968374958438971\n",
      "Accuracy on evaluation data: 9091 / 10000\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.1927739762669867\n",
      "Accuracy on training data: 4989 / 5000\n",
      "Cost on evaluation data: 0.8026877307940139\n",
      "Accuracy on evaluation data: 9076 / 10000\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.19126885299405832\n",
      "Accuracy on training data: 4989 / 5000\n",
      "Cost on evaluation data: 0.8051776246339764\n",
      "Accuracy on evaluation data: 9095 / 10000\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.18954269719972047\n",
      "Accuracy on training data: 4990 / 5000\n",
      "Cost on evaluation data: 0.7989667363976312\n",
      "Accuracy on evaluation data: 9093 / 10000\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.1888874631681694\n",
      "Accuracy on training data: 4989 / 5000\n",
      "Cost on evaluation data: 0.7990557819895008\n",
      "Accuracy on evaluation data: 9107 / 10000\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.1869982222024857\n",
      "Accuracy on training data: 4991 / 5000\n",
      "Cost on evaluation data: 0.8014263070161816\n",
      "Accuracy on evaluation data: 9099 / 10000\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.18445997202179665\n",
      "Accuracy on training data: 4991 / 5000\n",
      "Cost on evaluation data: 0.7986250381466368\n",
      "Accuracy on evaluation data: 9105 / 10000\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.18335839045459806\n",
      "Accuracy on training data: 4991 / 5000\n",
      "Cost on evaluation data: 0.8006166532220174\n",
      "Accuracy on evaluation data: 9100 / 10000\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.18255246029421113\n",
      "Accuracy on training data: 4991 / 5000\n",
      "Cost on evaluation data: 0.8005559712518127\n",
      "Accuracy on evaluation data: 9083 / 10000\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.1800905963519318\n",
      "Accuracy on training data: 4991 / 5000\n",
      "Cost on evaluation data: 0.7990470929831823\n",
      "Accuracy on evaluation data: 9110 / 10000\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.1789023237079353\n",
      "Accuracy on training data: 4992 / 5000\n",
      "Cost on evaluation data: 0.8029016174355446\n",
      "Accuracy on evaluation data: 9097 / 10000\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.17741975463265156\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.797564598512979\n",
      "Accuracy on evaluation data: 9108 / 10000\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.17573050592812528\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.8012104887919947\n",
      "Accuracy on evaluation data: 9111 / 10000\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.1743405547561802\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7924114375816865\n",
      "Accuracy on evaluation data: 9101 / 10000\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.17315494299923262\n",
      "Accuracy on training data: 4992 / 5000\n",
      "Cost on evaluation data: 0.7924146066779668\n",
      "Accuracy on evaluation data: 9112 / 10000\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.1722581776572281\n",
      "Accuracy on training data: 4992 / 5000\n",
      "Cost on evaluation data: 0.7923506224029685\n",
      "Accuracy on evaluation data: 9119 / 10000\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.17088576447101417\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.79032690106303\n",
      "Accuracy on evaluation data: 9117 / 10000\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.1693230107327736\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7912868300038995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 9112 / 10000\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.1677323860890745\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7903766079676818\n",
      "Accuracy on evaluation data: 9116 / 10000\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.16692858237637298\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7926626533570458\n",
      "Accuracy on evaluation data: 9116 / 10000\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.16558755644718656\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7933616229622065\n",
      "Accuracy on evaluation data: 9105 / 10000\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.16396295463262814\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7900946853372273\n",
      "Accuracy on evaluation data: 9113 / 10000\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.16296866674980262\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7910083092271212\n",
      "Accuracy on evaluation data: 9120 / 10000\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.16180124259512563\n",
      "Accuracy on training data: 4994 / 5000\n",
      "Cost on evaluation data: 0.7917346471890697\n",
      "Accuracy on evaluation data: 9103 / 10000\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.1605579657002198\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7862032546829507\n",
      "Accuracy on evaluation data: 9116 / 10000\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.15924002595723757\n",
      "Accuracy on training data: 4994 / 5000\n",
      "Cost on evaluation data: 0.7873158386925306\n",
      "Accuracy on evaluation data: 9120 / 10000\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.158290369750624\n",
      "Accuracy on training data: 4993 / 5000\n",
      "Cost on evaluation data: 0.7884996652799404\n",
      "Accuracy on evaluation data: 9128 / 10000\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.15743149951426963\n",
      "Accuracy on training data: 4994 / 5000\n",
      "Cost on evaluation data: 0.7846731100543759\n",
      "Accuracy on evaluation data: 9123 / 10000\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.1644185015050765\n",
      "Accuracy on training data: 4988 / 5000\n",
      "Cost on evaluation data: 0.7987435561694085\n",
      "Accuracy on evaluation data: 9107 / 10000\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.1544927854492817\n",
      "Accuracy on training data: 4995 / 5000\n",
      "Cost on evaluation data: 0.7864123549845304\n",
      "Accuracy on evaluation data: 9104 / 10000\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.15414958526911998\n",
      "Accuracy on training data: 4996 / 5000\n",
      "Cost on evaluation data: 0.7827483217367049\n",
      "Accuracy on evaluation data: 9117 / 10000\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.1525222227435474\n",
      "Accuracy on training data: 4996 / 5000\n",
      "Cost on evaluation data: 0.7890675079808207\n",
      "Accuracy on evaluation data: 9097 / 10000\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.15117323157694973\n",
      "Accuracy on training data: 4995 / 5000\n",
      "Cost on evaluation data: 0.7842268546016886\n",
      "Accuracy on evaluation data: 9114 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.3106159930648673,\n",
       "  1.0855912301437893,\n",
       "  0.9843839284571787,\n",
       "  0.9094653055005214,\n",
       "  0.8945695614612817,\n",
       "  0.8415109596027928,\n",
       "  0.8204983364947116,\n",
       "  0.8090360262976025,\n",
       "  0.8116160868923874,\n",
       "  0.8034750226751799,\n",
       "  0.7942653705778933,\n",
       "  0.8038803916433308,\n",
       "  0.7770778882762346,\n",
       "  0.7981207997530511,\n",
       "  0.7956941230539883,\n",
       "  0.7940044854286028,\n",
       "  0.7884098905590371,\n",
       "  0.7877065958214239,\n",
       "  0.8017391447527585,\n",
       "  0.8027758975024939,\n",
       "  0.7988330503141711,\n",
       "  0.7863208848437392,\n",
       "  0.8062766861606575,\n",
       "  0.8078216521889695,\n",
       "  0.7928126400822251,\n",
       "  0.8035341395445394,\n",
       "  0.8018307906259353,\n",
       "  0.8031647596743628,\n",
       "  0.7822509848715086,\n",
       "  0.7971092685492595,\n",
       "  0.7907430401115221,\n",
       "  0.7936968528534871,\n",
       "  0.8043262167429458,\n",
       "  0.7945733489275504,\n",
       "  0.7980267946316202,\n",
       "  0.794346412040628,\n",
       "  0.8000039174875897,\n",
       "  0.8080880198633662,\n",
       "  0.7984337229858933,\n",
       "  0.7959351427051463,\n",
       "  0.8023341130435928,\n",
       "  0.8010288177221987,\n",
       "  0.8019259984651419,\n",
       "  0.8045629180223859,\n",
       "  0.8033741505093722,\n",
       "  0.8024106536035334,\n",
       "  0.8021904936600364,\n",
       "  0.8032642073510604,\n",
       "  0.8046191778285002,\n",
       "  0.8070050533654469,\n",
       "  0.8122800394412028,\n",
       "  0.8043140809991849,\n",
       "  0.8018189786076194,\n",
       "  0.8069692524874178,\n",
       "  0.8058970639651274,\n",
       "  0.815121767091299,\n",
       "  0.8002173162835469,\n",
       "  0.8050256493357917,\n",
       "  0.8037406868163166,\n",
       "  0.8066608288823202,\n",
       "  0.8063111483398842,\n",
       "  0.8061608624360252,\n",
       "  0.7975745014823684,\n",
       "  0.8001368349905268,\n",
       "  0.8075102725400093,\n",
       "  0.8047378436694611,\n",
       "  0.7988063590169261,\n",
       "  0.7968374958438971,\n",
       "  0.8026877307940139,\n",
       "  0.8051776246339764,\n",
       "  0.7989667363976312,\n",
       "  0.7990557819895008,\n",
       "  0.8014263070161816,\n",
       "  0.7986250381466368,\n",
       "  0.8006166532220174,\n",
       "  0.8005559712518127,\n",
       "  0.7990470929831823,\n",
       "  0.8029016174355446,\n",
       "  0.797564598512979,\n",
       "  0.8012104887919947,\n",
       "  0.7924114375816865,\n",
       "  0.7924146066779668,\n",
       "  0.7923506224029685,\n",
       "  0.79032690106303,\n",
       "  0.7912868300038995,\n",
       "  0.7903766079676818,\n",
       "  0.7926626533570458,\n",
       "  0.7933616229622065,\n",
       "  0.7900946853372273,\n",
       "  0.7910083092271212,\n",
       "  0.7917346471890697,\n",
       "  0.7862032546829507,\n",
       "  0.7873158386925306,\n",
       "  0.7884996652799404,\n",
       "  0.7846731100543759,\n",
       "  0.7987435561694085,\n",
       "  0.7864123549845304,\n",
       "  0.7827483217367049,\n",
       "  0.7890675079808207,\n",
       "  0.7842268546016886],\n",
       " [8079,\n",
       "  8466,\n",
       "  8637,\n",
       "  8759,\n",
       "  8771,\n",
       "  8859,\n",
       "  8923,\n",
       "  8935,\n",
       "  8934,\n",
       "  8956,\n",
       "  8985,\n",
       "  8992,\n",
       "  8993,\n",
       "  8990,\n",
       "  8982,\n",
       "  9016,\n",
       "  9003,\n",
       "  9026,\n",
       "  9017,\n",
       "  9020,\n",
       "  9006,\n",
       "  9031,\n",
       "  9028,\n",
       "  9039,\n",
       "  9042,\n",
       "  9057,\n",
       "  9038,\n",
       "  9058,\n",
       "  9082,\n",
       "  9063,\n",
       "  9057,\n",
       "  9053,\n",
       "  9064,\n",
       "  9065,\n",
       "  9069,\n",
       "  9071,\n",
       "  9067,\n",
       "  9058,\n",
       "  9076,\n",
       "  9083,\n",
       "  9074,\n",
       "  9072,\n",
       "  9073,\n",
       "  9059,\n",
       "  9061,\n",
       "  9062,\n",
       "  9064,\n",
       "  9077,\n",
       "  9059,\n",
       "  9068,\n",
       "  9067,\n",
       "  9065,\n",
       "  9069,\n",
       "  9079,\n",
       "  9062,\n",
       "  9072,\n",
       "  9081,\n",
       "  9087,\n",
       "  9075,\n",
       "  9083,\n",
       "  9094,\n",
       "  9083,\n",
       "  9093,\n",
       "  9086,\n",
       "  9090,\n",
       "  9085,\n",
       "  9081,\n",
       "  9091,\n",
       "  9076,\n",
       "  9095,\n",
       "  9093,\n",
       "  9107,\n",
       "  9099,\n",
       "  9105,\n",
       "  9100,\n",
       "  9083,\n",
       "  9110,\n",
       "  9097,\n",
       "  9108,\n",
       "  9111,\n",
       "  9101,\n",
       "  9112,\n",
       "  9119,\n",
       "  9117,\n",
       "  9112,\n",
       "  9116,\n",
       "  9116,\n",
       "  9105,\n",
       "  9113,\n",
       "  9120,\n",
       "  9103,\n",
       "  9116,\n",
       "  9120,\n",
       "  9128,\n",
       "  9123,\n",
       "  9107,\n",
       "  9104,\n",
       "  9117,\n",
       "  9097,\n",
       "  9114],\n",
       " [1.284895665521856,\n",
       "  1.0055176989939365,\n",
       "  0.8536924837589502,\n",
       "  0.7550429847899467,\n",
       "  0.6960369940641402,\n",
       "  0.6381167275799088,\n",
       "  0.5773369380454864,\n",
       "  0.5518542263044559,\n",
       "  0.5269333373923851,\n",
       "  0.49692560760613136,\n",
       "  0.48830731774508096,\n",
       "  0.4747061821230185,\n",
       "  0.4300847789579307,\n",
       "  0.4320789613225059,\n",
       "  0.4218290287068238,\n",
       "  0.39554494682486696,\n",
       "  0.3814449259649054,\n",
       "  0.37409461543905426,\n",
       "  0.3661659404661851,\n",
       "  0.3571608166631301,\n",
       "  0.35442988574061696,\n",
       "  0.34033121288269363,\n",
       "  0.3404149388157005,\n",
       "  0.33691583855508334,\n",
       "  0.31924086626382475,\n",
       "  0.3232712795012312,\n",
       "  0.32056215329090326,\n",
       "  0.30648091472431005,\n",
       "  0.2967405359204987,\n",
       "  0.29503943802803223,\n",
       "  0.29213008487063724,\n",
       "  0.2875801502380281,\n",
       "  0.28656812403820026,\n",
       "  0.2773597726670703,\n",
       "  0.2750230398337855,\n",
       "  0.2719899184710543,\n",
       "  0.2691230281521867,\n",
       "  0.2663643738569248,\n",
       "  0.2606307914361742,\n",
       "  0.2577303229775922,\n",
       "  0.2546552137479804,\n",
       "  0.25152966927047427,\n",
       "  0.2494053444198024,\n",
       "  0.24750876087574938,\n",
       "  0.24400801383099724,\n",
       "  0.24063322736834436,\n",
       "  0.23730865314686692,\n",
       "  0.2351534627384445,\n",
       "  0.2343316244177092,\n",
       "  0.23073728346985986,\n",
       "  0.2280545958944444,\n",
       "  0.22540541821570115,\n",
       "  0.22280003375165358,\n",
       "  0.22171997482629113,\n",
       "  0.2182822118429678,\n",
       "  0.21770652907544058,\n",
       "  0.21499548439312924,\n",
       "  0.2126100086564361,\n",
       "  0.21043675859431993,\n",
       "  0.2085881785205787,\n",
       "  0.20667396906605495,\n",
       "  0.2050870999781125,\n",
       "  0.20377729929720967,\n",
       "  0.20109625078800397,\n",
       "  0.2006278480087793,\n",
       "  0.19760283223756947,\n",
       "  0.19605053493004432,\n",
       "  0.19424954022558166,\n",
       "  0.1927739762669867,\n",
       "  0.19126885299405832,\n",
       "  0.18954269719972047,\n",
       "  0.1888874631681694,\n",
       "  0.1869982222024857,\n",
       "  0.18445997202179665,\n",
       "  0.18335839045459806,\n",
       "  0.18255246029421113,\n",
       "  0.1800905963519318,\n",
       "  0.1789023237079353,\n",
       "  0.17741975463265156,\n",
       "  0.17573050592812528,\n",
       "  0.1743405547561802,\n",
       "  0.17315494299923262,\n",
       "  0.1722581776572281,\n",
       "  0.17088576447101417,\n",
       "  0.1693230107327736,\n",
       "  0.1677323860890745,\n",
       "  0.16692858237637298,\n",
       "  0.16558755644718656,\n",
       "  0.16396295463262814,\n",
       "  0.16296866674980262,\n",
       "  0.16180124259512563,\n",
       "  0.1605579657002198,\n",
       "  0.15924002595723757,\n",
       "  0.158290369750624,\n",
       "  0.15743149951426963,\n",
       "  0.1644185015050765,\n",
       "  0.1544927854492817,\n",
       "  0.15414958526911998,\n",
       "  0.1525222227435474,\n",
       "  0.15117323157694973],\n",
       " [4197,\n",
       "  4419,\n",
       "  4548,\n",
       "  4621,\n",
       "  4665,\n",
       "  4735,\n",
       "  4786,\n",
       "  4804,\n",
       "  4822,\n",
       "  4838,\n",
       "  4846,\n",
       "  4855,\n",
       "  4883,\n",
       "  4877,\n",
       "  4889,\n",
       "  4915,\n",
       "  4929,\n",
       "  4925,\n",
       "  4934,\n",
       "  4927,\n",
       "  4941,\n",
       "  4944,\n",
       "  4945,\n",
       "  4950,\n",
       "  4955,\n",
       "  4951,\n",
       "  4948,\n",
       "  4965,\n",
       "  4963,\n",
       "  4966,\n",
       "  4970,\n",
       "  4966,\n",
       "  4967,\n",
       "  4973,\n",
       "  4973,\n",
       "  4975,\n",
       "  4973,\n",
       "  4975,\n",
       "  4976,\n",
       "  4977,\n",
       "  4977,\n",
       "  4978,\n",
       "  4978,\n",
       "  4978,\n",
       "  4980,\n",
       "  4980,\n",
       "  4985,\n",
       "  4986,\n",
       "  4984,\n",
       "  4985,\n",
       "  4985,\n",
       "  4985,\n",
       "  4987,\n",
       "  4986,\n",
       "  4986,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4988,\n",
       "  4989,\n",
       "  4989,\n",
       "  4989,\n",
       "  4989,\n",
       "  4989,\n",
       "  4990,\n",
       "  4989,\n",
       "  4991,\n",
       "  4991,\n",
       "  4991,\n",
       "  4991,\n",
       "  4991,\n",
       "  4992,\n",
       "  4993,\n",
       "  4993,\n",
       "  4993,\n",
       "  4992,\n",
       "  4992,\n",
       "  4993,\n",
       "  4993,\n",
       "  4993,\n",
       "  4993,\n",
       "  4993,\n",
       "  4993,\n",
       "  4993,\n",
       "  4994,\n",
       "  4993,\n",
       "  4994,\n",
       "  4993,\n",
       "  4994,\n",
       "  4988,\n",
       "  4995,\n",
       "  4996,\n",
       "  4996,\n",
       "  4995])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "test_data = list(test_data)\n",
    "validation_data = list(validation_data)\n",
    "training_data = list(training_data)\n",
    "\n",
    "net = Network([784, 30, 10], cost=CrossEntropyCost) \n",
    "net.large_weight_initializer()\n",
    "\n",
    "net.SGD(training_data[:5000], 100, 10, 0.5, evaluation_data=test_data, lmbda = 0.1,monitor_evaluation_cost=True, monitor_evaluation_accuracy=True, monitor_training_cost=True, monitor_training_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works very well, even though it takes a while. But why ? Why are small weights better ? In what sense are they \"less complex\" ? \n",
    "\n",
    "Here's one way of seing this: when weights are small, changing a few inputs by a little bit at random won't have a huge effect on the network's output. This means that the network is robust, that it doen't contort its parameters to fit the data's noise. Note to self: use the word 'procrustean' somewhere here. \n",
    "\n",
    "Although Occam's razor is useful in many situations, it isn't a law of nature: sometimes the more complex answer can be correct (*eg* sometimes polynomal interpolation is preferable to linear regression). We apparently don't have a fully fledged theory of why regularization works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### $L^1$ regularization\n",
    "\n",
    "We add a portion of the $L^1$ norm of the weight vector, instead of a portion of the square of the $L^2$ norm. The new cost function is:\n",
    "\n",
    "$$C = C_0 + \\frac \\lambda n \\sum_w |w|$$\n",
    "\n",
    "The weights are updated using the derivative of the previous formula (except when it isn't differentiable, in which case the unregularized rule is used):\n",
    "\n",
    "$$\\frac {\\partial C} {\\partial w} = \\frac {\\partial C_0} {\\partial w} + \\frac \\lambda n sgn(w)$$\n",
    "\n",
    "In both $L^1$ and $L^2$ regularization, every iteration reduces the value of the weights, but by a different amount. In $L^2$ regularization, this amount depends on the value of the weight being updated, whereas in $L^1$ regularization, this amount is always the same.\n",
    "\n",
    "Therefore $L^2$ has a uniformizing effect on the weights: bigger weights get \"taxed\" more than small weights. On the other hand, $L^1$ changes every weight by the same amount, and leads to a more irregular distribution of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Dropout\n",
    "\n",
    "Dropout is a completely different technique, that doesn't involve tweaking the cost function. We use stochastic gradient descent, except for every mini-batch only part of the network is trained: some hidden neurons are \"ghosted\", propagation/backpropagation happens only on the remaining nodes. For every new mini-batch, a new set of hidden nodes is left out.\n",
    "\n",
    "Intuitively, this is very close to training different networks on the same data and choosing the output as the average of their outputs, or by making them \"vote\" for the right answer.\n",
    "\n",
    "Since at some point during the training, neurons will be learning while their neighours are \"ghosted\", the network is in a way trained to be robust with regards to changes in the output of nearby neurons. This is a good way of avoiding overfitting. Dropout works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Artificially expanding the training data\n",
    "\n",
    "If we add to the original training data pictures obtained by rotating (and/or translating, skewing) training pictures, we get more data (nearly) for free and make our network more robust with respect to these kinds of image transformations. \n",
    "Mike says: **\"The general principle is to expand the training data by applying operations that reflect real-world variation.\"**\n",
    "\n",
    "If we allow too large rotations of MNIST images, the network may end up producing nonsense. For instance, it may start confusing $6$ and $9$.\n",
    "\n",
    "\n",
    "### An aside on comparing algorithms\n",
    "\n",
    "An algorithm can be better than another on a certain dataset, and worse on others. So we should be wary of overly spectacular allegations regarding any given technique: for example, some algorithms are extremely efficient only to solve a very restricted set of problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Initializing the weight distribution \n",
    "Up to now, we've initialized or weights and biases at random, using a normal distribution:\n",
    "$$\\forall l, \\forall j, \\forall k, w^l_{jk} \\sim \\mathcal{N}(0, 1)$$\n",
    "$$\\forall l, \\forall j, b^l_{j} \\sim \\mathcal{N}(0, 1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the problem with that ? \n",
    "\n",
    "If the initial weights all follow this distribution, then the weighted input $z$ of any given neuron in the first layer is quite likely to be very big or very small, $|z| >> 1 $.\n",
    "\n",
    "For instance, if the input layer contains $1000$ neurons, for an input $x$ such that $x_i = 1$ for $500$ values of $i$ and $x_i = 0$ for the $500$ others, the weighted input $z$ will follow a centered normal distribution with a standard error close to $22.4$:\n",
    "\n",
    "$$z = \\sum^{500}_{i = 1} w_i + b_i \\sim \\mathcal{N}(0, 501 \\simeq 22.4^2)$$\n",
    "\n",
    "It is therefore quite probable that $|z| >> 1$, meaning that $\\sigma'(z) \\simeq 0$: as we saw previously in this chapter, this causes a learning slowdown. This kind of argument generalizes to weights in other layers: the idea is the same but the computations get more ugly.\n",
    "\n",
    "### What's a better choice ?\n",
    "\n",
    "Let $n_in$ be the number of input weights on a neuron. Then the weights toward that neuron should be initialized following a normal distribution of standard error $\\frac {1} {\\sqrt n_{in}}$.\n",
    "\n",
    "This way, with the same input as above, ie $n_{in} = 1000$:\n",
    "$$z = \\sum^{500}_{i = 1} w_i + b_i \\sim \\mathcal{N}(0, 1 + \\sum^{500}_{i = 1} \\frac {1}{n_{in}}) = \\mathcal{N}(0, 1 + \\frac 1 2) = \\mathcal{N}(0, \\frac 3 2)$$\n",
    "\n",
    "The standard deviation is $\\sqrt{\\frac 3 2 }$so that it is far less likely that $|z| >> 1$.\n",
    "\n",
    "Biases don't matter too much, so we'll keep initializing them according to $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this achieve in practice ?\n",
    "\n",
    "Our original approach to initialization can be compared to the one given above in `network2.py` file (or the first Jupyter notebook corresponding to this chapter) by using either the `default_weight_initializer` or the `large_weight_initializer` methods.\n",
    "\n",
    "Though the final accuracy is pretty similar, the network learns much faster when the weights are initialized with this newer method. There are other example of networks where both the speed and the final accuracy are improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Choosing hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of hyperparameters:\n",
    "* the learning rate $\\eta$\n",
    "* the regularization parameter $\\lambda$\n",
    "* mini-batch size\n",
    "* number of neurons\n",
    "* number of epochs\n",
    "\n",
    "\n",
    "When a given choice of hyperparameters sucks, it's hard to know which one to change, and what change to make. This section contains a few tips, tricks and heuristics though there isn't a formal consensus on **the** perfect method. In practice hyperparameter choice is often automated, for example by performing grid search in hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simplify things and make sure you're better than chance\n",
    "\n",
    "Making sure your network is better than chance for a **simpler task** is an important step: testing and tweaking will often be less time consuming as your training set will be smaller, and at least guarantees that you're going in the right direction.\n",
    "\n",
    "**Simplifying the network** is useful for similar reasons, as is getting **more frequent feedback** on its performance. All these techniques give us leeway to try more things out, some space to experiments. In general, the hyperparameter choice it leads won't do drastically worse with a more complex network tackling a tougher task.\n",
    "\n",
    "So you'll often have to fumble, changing one parameter after another and progressively improving your results. This is why feedback is so important.\n",
    "\n",
    "Such is the general idea. Next we'll see some more strategies regarding specific hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The learning rate $\\eta$\n",
    "\n",
    "If $\\eta$ is too:\n",
    "* big: the cost will oscillate a lot during learning, without really decreasing\n",
    "* small: the cost will decrease, but slowly\n",
    "\n",
    "A decent strategy to get to the sweet spot is therefore to increase $\\eta$ while the cost is decreasing during the first epochs, until it starts oscillating.\n",
    "\n",
    "\n",
    "The learning rate is often variable (as it is in other uses of gradient descent). There are many different methods and heuristics (see https://en.wikipedia.org/wiki/Line_search) to choose where along the direction of the gradient to set the netx point. A general idea is to have $\\eta$ rather big in the beginning and getting smaller progressively, changing in value when accuracy stops increasing. In this case the learning rate's size can be a condition for stopping: learning ceases whenever it's $x$ times smaller than its initial value.\n",
    "\n",
    "Using gradient descent to find a good value for $\\eta$ at each step is theoretically possible: it involves the minimisation of the function $[h \\mapsto C(w - h \\frac {\\partial C} {\\partial w})]$. Assuming this method can be useful for theoretical proofs of convergence, but in practice it's probably too time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Number of epochs\n",
    "\n",
    "The general idea here is to **stop when the network stops improving** (trating from now, we'll assume we're testing the network on the validation data to set hyperparameters). This is also useful to avoid overfitting.\n",
    "\n",
    "There are subtleties regarding what is meant by \"the network stops improving\", that should be handled on a case by case basis, for instance according to how much accuracy we are asking relative to speed.\n",
    "\n",
    "You can use a \"no improvement in ten\" rule (stop when the accuracy hasn't increased in the ten last epochs) when you're starting to set the hyperparameters, to weed out obviously bad choices. As you narrow down on decent hyperparameter values, it becomes more reasonable to give the network the benefit of the doubt if it doesn't improve much for a few consecutive epochs: maybe the training data was unlucky, maybe the plateau is only temporary...\n",
    "\n",
    "To implement early stopping in ``network2.py``, we modify the ``SGD`` method in the ``Network`` class: instead of using a ``for`` loop we can use a conditional ``while`` loop, with the condition to check is whether the the current best accuracy is within the $n$ last elements in the  ``evaluation_accuracy`` list. This entails keeping track of the current best accuracy, which can be done through a ``current_best_accuracy`` variable updated if need be at each iteration.\n",
    "\n",
    "Another plausible rule for early stoppin would be to compare the current accuracy to the average of the $n$ previous accuracies, and to stop if the difference is small enough. In the same spirit, the network could stop when the variance in the distribution of past accuracies is smaller than some small preset parameter $\\epsilon > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The $L^2$ regularization parameter $\\lambda$\n",
    "\n",
    "First we use the training data to select a good value for $\\eta$, without regularization. Then we can use the validation data to find an order of magnitude for $\\lambda$, starting at $\\lambda = 1.0$ (arbitrary, apparently). Then tweak it a little bit, and then it's back and forth between $\\eta$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The mini-batch size\n",
    "\n",
    "Small mini-batches (the extreme case being online learning) pros and cons:\n",
    "* pros: regular updating, the lesser precision isn't a huge issue (Nielsen's inaccurate compass analogy is great)\n",
    "* cons: doesn't take advantage of the faster, matrix based implementation of SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Alternatives to $SGD$\n",
    "\n",
    "* **Newton's method aka Hessian optimisation** is faster in theory than Gradient descent since it uses second order information on the cost function to determine the next point (it works assuming local convexity). However it involves computing the (huge) Hessian matrix at each step which is expensive. BFGS is a good algorithm that works similarly, but efficiently computes an approximation of the Hessian.\n",
    "\n",
    "$$w \\to w' = w - \\eta H^{-1}\\nabla C $$\n",
    "\n",
    "* **Momentum based Gradient Descent** is a modification of Gradient descent with new variables for each weight/bias: velocities. The gradient only has a direct influence on the velocities, and the velocities determine the rates of change of the weights and biases. \n",
    "\n",
    "$$v \\to v' = \\mu v - \\eta \\nabla C$$\n",
    "$$w \\to w' = w + v' $$\n",
    "\n",
    " If $\\mu > 1$ velocities and therefore weights will tend to explose.\n",
    " If $\\mu < 0$ there will be many useless comings and goings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Other neuron models\n",
    "The difference between these is their activation function. Though we can have some domain specific mathematically informed heuristics, the choice of an appropriate neuron model is to a large extent an empirical matter.\n",
    "\n",
    "* $tanh$ **neuron**\n",
    "\n",
    "It's more or less the same thing than the sigmoid neuron, since:\n",
    "\n",
    "$$\\forall x \\in \\mathbb{R}, tanh(x) = \\frac {e^{x} - e^{-x}} {e^{-x} + e^{x}} = \\frac {1 - e^{-2x}} {1 + e^{-2x}} = \\sigma(2x) - [1 - \\sigma(2x)] = 2\\times \\sigma(2x)-1$$\n",
    "\n",
    "* **rectified linear neuron**\n",
    "\n",
    "The activation function here is $[z \\mapsto max(0,z)]$. These can be useful for image recognition. It's as universal (in the sense of being able to approximate any function under reasonable conditions) as the sigmoid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
