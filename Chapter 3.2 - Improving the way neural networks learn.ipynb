{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. $L^2$ regularization\n",
    "\n",
    "As we saw previously, overfitting is what happens when variance is too big. Regularization is a way to solve this issue without impeding the network's power too much: the idea is to penalize model complexity by modifying the cost function.\n",
    "\n",
    "\n",
    "We therefore introduce the following cost:\n",
    "$$C = C_0 + \\frac {\\lambda} {2n} \\sum_w w^2$$\n",
    "where $\\lambda \\in \\mathbb{R}+$ is called the **regularization parameter**, and $C_0$ is a non-regularized cost function. For instance if $C_0$ is the cross entropy cost function:\n",
    "$$C = - \\frac {1} {n} \\sum_x yln(a) + (1-y)ln(1-a) + \\frac {\\lambda} {2n} \\sum_w w^2$$\n",
    "\n",
    "And when $C_0$ is the quadratic cost function:\n",
    "$$C = - \\frac {1} {n} \\sum_x ||y(x)-a||^2 + \\frac {\\lambda} {2n} \\sum_w w^2$$\n",
    "\n",
    "$\\lambda$ represents whether we are more concerned with bias or variance. Notice that we haven't yet shown how exactly penalizing big weights is tantamount to avoiding excessive model complexity.\n",
    "\n",
    "Regularization doesn't make gradient descent much more complicated: the rate of change of $C_0$ with respect to the weights and biases can be computed using backpropagation, and the gradient is given by $\\frac {\\partial C} {\\partial w} = \\frac {\\partial C_0} {\\partial w} + \\frac {\\lambda w} {n}$ and $\\frac {\\partial C} {\\partial b} = \\frac {\\partial C_0} {\\partial b}$. Stochastic gradient descent also happens naturally (for weights, approximate the gradient of $C_0$ and then add the $\\frac {\\lambda w} {n}$ term, for biases, regularization doen't change anything).\n",
    "\n",
    "### Testing $L^2$ regularization\n",
    "Let's test these changes on the neural network that we used in the previous part to showcase the ills of overfitting. As often, this code is Michael Nielsen's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what's in ``mnist_loader.py``, *i.e.* the functions that load MNIST the right way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('NNDL_file/data/mnist.pkl.gz')\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again to import the data I had to tinker a little bit with the code so that it would work in Python 3:\n",
    "* changed ``cPickle`` into ``Pickle``\n",
    "* converted ``test_data``, ``validation_data``, and ``training_data`` into lists for them to be iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 3.0060716893140866\n",
      "Accuracy on training data: 666 / 1000\n",
      "Cost on evaluation data: 2.18005437332356\n",
      "Accuracy on evaluation data: 5841 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 2.54092158429232\n",
      "Accuracy on training data: 764 / 1000\n",
      "Cost on evaluation data: 1.8414377793742103\n",
      "Accuracy on evaluation data: 6712 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 2.2430160622918707\n",
      "Accuracy on training data: 834 / 1000\n",
      "Cost on evaluation data: 1.6006331858289273\n",
      "Accuracy on evaluation data: 7308 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 2.062198517273762\n",
      "Accuracy on training data: 867 / 1000\n",
      "Cost on evaluation data: 1.5209499433214697\n",
      "Accuracy on evaluation data: 7492 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.9181422659472878\n",
      "Accuracy on training data: 901 / 1000\n",
      "Cost on evaluation data: 1.4444900044007924\n",
      "Accuracy on evaluation data: 7669 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.7789700822964503\n",
      "Accuracy on training data: 921 / 1000\n",
      "Cost on evaluation data: 1.3699484506404698\n",
      "Accuracy on evaluation data: 7843 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.7051154261678034\n",
      "Accuracy on training data: 930 / 1000\n",
      "Cost on evaluation data: 1.3775040500670102\n",
      "Accuracy on evaluation data: 7799 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.6031491428879336\n",
      "Accuracy on training data: 949 / 1000\n",
      "Cost on evaluation data: 1.296045614181941\n",
      "Accuracy on evaluation data: 7983 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.5499152784399461\n",
      "Accuracy on training data: 955 / 1000\n",
      "Cost on evaluation data: 1.2874623100901372\n",
      "Accuracy on evaluation data: 8014 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.5130582825105705\n",
      "Accuracy on training data: 960 / 1000\n",
      "Cost on evaluation data: 1.3220000327257853\n",
      "Accuracy on evaluation data: 7950 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1.450594120551548\n",
      "Accuracy on training data: 966 / 1000\n",
      "Cost on evaluation data: 1.2904524972238294\n",
      "Accuracy on evaluation data: 8032 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1.4020210530973038\n",
      "Accuracy on training data: 971 / 1000\n",
      "Cost on evaluation data: 1.2710135050571383\n",
      "Accuracy on evaluation data: 8066 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1.3591668519298048\n",
      "Accuracy on training data: 980 / 1000\n",
      "Cost on evaluation data: 1.2691451933163893\n",
      "Accuracy on evaluation data: 8092 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1.3229460438378058\n",
      "Accuracy on training data: 983 / 1000\n",
      "Cost on evaluation data: 1.239784002138338\n",
      "Accuracy on evaluation data: 8160 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1.2984977923390066\n",
      "Accuracy on training data: 986 / 1000\n",
      "Cost on evaluation data: 1.2369192107206346\n",
      "Accuracy on evaluation data: 8146 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 1.2676463738075792\n",
      "Accuracy on training data: 989 / 1000\n",
      "Cost on evaluation data: 1.2460478826164818\n",
      "Accuracy on evaluation data: 8143 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 1.2413639320729222\n",
      "Accuracy on training data: 988 / 1000\n",
      "Cost on evaluation data: 1.2364298619377339\n",
      "Accuracy on evaluation data: 8153 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 1.2247794801179352\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.2739513158692368\n",
      "Accuracy on evaluation data: 8127 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 1.200039500871057\n",
      "Accuracy on training data: 990 / 1000\n",
      "Cost on evaluation data: 1.2383467706687563\n",
      "Accuracy on evaluation data: 8182 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 1.1800388668489772\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.2480680453986694\n",
      "Accuracy on evaluation data: 8193 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 1.1631260866539304\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.2334656510316115\n",
      "Accuracy on evaluation data: 8187 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 1.1462849017027663\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.2593859305219164\n",
      "Accuracy on evaluation data: 8192 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 1.1302608268456011\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.2497879966906364\n",
      "Accuracy on evaluation data: 8186 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 1.115178739455214\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.2493450050915866\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 1.1004472906963727\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 1.2567884248207866\n",
      "Accuracy on evaluation data: 8218 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 1.0868814918341172\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 1.2489206237276627\n",
      "Accuracy on evaluation data: 8231 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 1.072438655522772\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.252612530865387\n",
      "Accuracy on evaluation data: 8235 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 1.0637493372086149\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2656917771399225\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 1.046701988382645\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2556864548080107\n",
      "Accuracy on evaluation data: 8251 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 1.0342797594150988\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2518071945104305\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 1.0227763950781106\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2508931460081483\n",
      "Accuracy on evaluation data: 8248 / 10000\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 1.0103345860671447\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.247638884460392\n",
      "Accuracy on evaluation data: 8248 / 10000\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.9985817812825458\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2539351269631183\n",
      "Accuracy on evaluation data: 8265 / 10000\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.9881670203118029\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2563663067193647\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.9786243749056126\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.2548983217525371\n",
      "Accuracy on evaluation data: 8254 / 10000\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.9668144224784035\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2608830817917844\n",
      "Accuracy on evaluation data: 8262 / 10000\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.9568231175748116\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2530303359531385\n",
      "Accuracy on evaluation data: 8260 / 10000\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.9476604677520161\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2570957010563941\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.9372619089848018\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2506947143826952\n",
      "Accuracy on evaluation data: 8270 / 10000\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.9273768965208723\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2574178702084502\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.9189490292140069\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2588529175931276\n",
      "Accuracy on evaluation data: 8262 / 10000\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.9091614404165043\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2545883811549439\n",
      "Accuracy on evaluation data: 8263 / 10000\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.8996996833129095\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2489337253214268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 8284 / 10000\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.8913493198459363\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2534133400276437\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.8828600219621082\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.252349750788906\n",
      "Accuracy on evaluation data: 8279 / 10000\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.8745044276559535\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.244762700442233\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.8653764627252684\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2510397528218107\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.8575386110261546\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2509835562663014\n",
      "Accuracy on evaluation data: 8282 / 10000\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.849022843855989\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.244144285262516\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.8412297298561927\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2501373963859248\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.8335319992649615\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2525213082096722\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.8254226231707492\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2470969062305635\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.817942593658961\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2394641486915368\n",
      "Accuracy on evaluation data: 8309 / 10000\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.810507132585183\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.239787969390123\n",
      "Accuracy on evaluation data: 8303 / 10000\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.8025038375480188\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2418908473019483\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.7957885155069165\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2401750879871953\n",
      "Accuracy on evaluation data: 8317 / 10000\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.7883706610445915\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2320147765022278\n",
      "Accuracy on evaluation data: 8316 / 10000\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.7809185212503931\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2357375297943263\n",
      "Accuracy on evaluation data: 8306 / 10000\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.7736837103786818\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2338158489582904\n",
      "Accuracy on evaluation data: 8306 / 10000\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.7669533699268576\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.231361592560095\n",
      "Accuracy on evaluation data: 8323 / 10000\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.7597870929087863\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2359959790739108\n",
      "Accuracy on evaluation data: 8321 / 10000\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.7536498610083385\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2389849755726885\n",
      "Accuracy on evaluation data: 8310 / 10000\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.7467234718945737\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2250118842885847\n",
      "Accuracy on evaluation data: 8319 / 10000\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.7395139667266182\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.229294373200715\n",
      "Accuracy on evaluation data: 8325 / 10000\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.7329928207827431\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2267321239075009\n",
      "Accuracy on evaluation data: 8320 / 10000\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.7267745012659029\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.234139408615249\n",
      "Accuracy on evaluation data: 8322 / 10000\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.719965329256834\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2209820875747324\n",
      "Accuracy on evaluation data: 8334 / 10000\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.713788403805119\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2209810903668532\n",
      "Accuracy on evaluation data: 8330 / 10000\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.7073562929453762\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2186049130828238\n",
      "Accuracy on evaluation data: 8335 / 10000\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.7011396708314184\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.214765273090068\n",
      "Accuracy on evaluation data: 8347 / 10000\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.6952127606612944\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2169061308218931\n",
      "Accuracy on evaluation data: 8343 / 10000\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.6894470008249196\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.222349058861027\n",
      "Accuracy on evaluation data: 8326 / 10000\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.6832167506309011\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.20897692550518\n",
      "Accuracy on evaluation data: 8358 / 10000\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.6772039085036824\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2080468391070276\n",
      "Accuracy on evaluation data: 8348 / 10000\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.6714653959423679\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2042855082911124\n",
      "Accuracy on evaluation data: 8355 / 10000\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.6657133580959097\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2056710732717635\n",
      "Accuracy on evaluation data: 8352 / 10000\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.6600533560315209\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2023381502773811\n",
      "Accuracy on evaluation data: 8354 / 10000\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.6545567843819986\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.200872658179037\n",
      "Accuracy on evaluation data: 8344 / 10000\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.6486741362051606\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1956412872166997\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.643120228439617\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.195448467739748\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.6376708839787878\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1928656637290347\n",
      "Accuracy on evaluation data: 8363 / 10000\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.6322906189052175\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1928069399464676\n",
      "Accuracy on evaluation data: 8357 / 10000\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.627011081813797\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1891073162080463\n",
      "Accuracy on evaluation data: 8365 / 10000\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.6216638579175982\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1887640952837124\n",
      "Accuracy on evaluation data: 8369 / 10000\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.6165394127277792\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.18955099517358\n",
      "Accuracy on evaluation data: 8372 / 10000\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.6113195198974527\n",
      "Accuracy on training data: 1000 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 1.1856072089004328\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.6064653506048084\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.191672966508033\n",
      "Accuracy on evaluation data: 8365 / 10000\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.6012339538848578\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1763113963829985\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.5959361064940054\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1789216789927872\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.5910999998013722\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1769141506321923\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.5861901323872261\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1732720333878008\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.5815240751681239\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.172375658306194\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.5765325677172686\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1676800784656878\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.5719520813678235\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1688497847516521\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.5670594916641896\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1712462395265988\n",
      "Accuracy on evaluation data: 8404 / 10000\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.5623340912040485\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1624685623399709\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.5578617890642404\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1590945823525598\n",
      "Accuracy on evaluation data: 8405 / 10000\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.5530032354234885\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1602468508485941\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.5483958348548059\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1581036678682286\n",
      "Accuracy on evaluation data: 8408 / 10000\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.5437980537864178\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.156041689403608\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.5394532681477959\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1491502139719916\n",
      "Accuracy on evaluation data: 8409 / 10000\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.5350182071311191\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.148347447802808\n",
      "Accuracy on evaluation data: 8407 / 10000\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.5308727330286663\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1487938973232532\n",
      "Accuracy on evaluation data: 8418 / 10000\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.5263887100776572\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1494625414465525\n",
      "Accuracy on evaluation data: 8409 / 10000\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.5220362982244812\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1415039586821238\n",
      "Accuracy on evaluation data: 8415 / 10000\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.5177878649015966\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1436438424366373\n",
      "Accuracy on evaluation data: 8410 / 10000\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.5135715329880064\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1392649057694695\n",
      "Accuracy on evaluation data: 8425 / 10000\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.5095706830856509\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1398605372387147\n",
      "Accuracy on evaluation data: 8424 / 10000\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.5055857090002279\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1373389395658382\n",
      "Accuracy on evaluation data: 8426 / 10000\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.5016337102562474\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1400014862964416\n",
      "Accuracy on evaluation data: 8421 / 10000\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.49744762276306187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1304194920668396\n",
      "Accuracy on evaluation data: 8437 / 10000\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.4934800986038513\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1278019093608775\n",
      "Accuracy on evaluation data: 8437 / 10000\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.4894403237197677\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.130927283442431\n",
      "Accuracy on evaluation data: 8433 / 10000\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.4855798254741585\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1267992265893034\n",
      "Accuracy on evaluation data: 8428 / 10000\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.4818142806329021\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.120162542493072\n",
      "Accuracy on evaluation data: 8440 / 10000\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.4779428428309343\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1227055127589627\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.4742254692190958\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1209821474667592\n",
      "Accuracy on evaluation data: 8427 / 10000\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.470445215544694\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.114077190862701\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.4668319756194084\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.111903159683744\n",
      "Accuracy on evaluation data: 8440 / 10000\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.4630536666057951\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1129554608138916\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.45941951731481956\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1095791476054455\n",
      "Accuracy on evaluation data: 8448 / 10000\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.45580938096892254\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1100404146740808\n",
      "Accuracy on evaluation data: 8442 / 10000\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.45224751188283785\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1078827760382064\n",
      "Accuracy on evaluation data: 8442 / 10000\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.44879064463011215\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1065779891691145\n",
      "Accuracy on evaluation data: 8443 / 10000\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.44523516130994806\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1033461230544734\n",
      "Accuracy on evaluation data: 8441 / 10000\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.4418386301620734\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1013861634534643\n",
      "Accuracy on evaluation data: 8450 / 10000\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.43859861826046614\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.102663515823086\n",
      "Accuracy on evaluation data: 8459 / 10000\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.4351338257696293\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0981501448107722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 8452 / 10000\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.4317308259318641\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0985920203487902\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.4283925756257805\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0953257824728972\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.42532688597889134\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.093028218634507\n",
      "Accuracy on evaluation data: 8464 / 10000\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.4217997473343187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0882341177114774\n",
      "Accuracy on evaluation data: 8470 / 10000\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.4186966189749271\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0861462349524331\n",
      "Accuracy on evaluation data: 8463 / 10000\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.41555296975076056\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0797026360959974\n",
      "Accuracy on evaluation data: 8475 / 10000\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.41252524822566455\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0900465924841891\n",
      "Accuracy on evaluation data: 8459 / 10000\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.4092190991011243\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0823007459168918\n",
      "Accuracy on evaluation data: 8466 / 10000\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.40611506089458904\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.079833518403186\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.40315921810782895\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.078554077630273\n",
      "Accuracy on evaluation data: 8475 / 10000\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.40004266967856866\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0735058431761564\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.39724340582476286\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0768828774263592\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.39415036863936187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0731233147468635\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.39122856669557693\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0731050825554782\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.3884784271427704\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.075002467158796\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.38540701000025046\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0672173306245125\n",
      "Accuracy on evaluation data: 8485 / 10000\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.3825654181389158\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0646665006387501\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.3798195261312202\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.066495928769525\n",
      "Accuracy on evaluation data: 8485 / 10000\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.37692239635342734\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0631218192327887\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.37431523438198366\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0575534234808417\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.3716533207663261\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0615000601163291\n",
      "Accuracy on evaluation data: 8493 / 10000\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.36878646790442715\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0574988456736523\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.3661909942689351\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.055601032354897\n",
      "Accuracy on evaluation data: 8508 / 10000\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.36359965907956976\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0546713736725968\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.3608628384573427\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0483282912528282\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.3583093300654738\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0519228619350558\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.35582223163278526\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0533618243205862\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.35315270319762676\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.047236292121468\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.3506323970608068\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0428797925461737\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.34850976962848484\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0440970272143877\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.34563737715945037\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0430389764341002\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.3430815940756603\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0370906816637817\n",
      "Accuracy on evaluation data: 8524 / 10000\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.3409163528063585\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0407633635047777\n",
      "Accuracy on evaluation data: 8524 / 10000\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.3385353007606962\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0423259712911244\n",
      "Accuracy on evaluation data: 8507 / 10000\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.3359718495659589\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0332587587297206\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.3335587558541371\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0302910283121431\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.3314808808366025\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0310250350770351\n",
      "Accuracy on evaluation data: 8534 / 10000\n",
      "Epoch 165 training complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-88c0ade2a83a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlarge_weight_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonitor_evaluation_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_evaluation_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_training_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_training_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-a219134fc236>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, lmbda, evaluation_data, monitor_evaluation_cost, monitor_evaluation_accuracy, monitor_training_cost, monitor_training_accuracy)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %s training complete\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmonitor_training_cost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mtraining_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cost on training data: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a219134fc236>\u001b[0m in \u001b[0;36mtotal_cost\u001b[0;34m(self, data, lmbda, convert)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a219134fc236>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m\"\"\"Return the output of the network if ``a`` is input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "test_data = list(test_data)\n",
    "validation_data = list(validation_data)\n",
    "training_data = list(training_data)\n",
    "\n",
    "net = Network([784, 30, 10], cost=CrossEntropyCost) \n",
    "net.large_weight_initializer()\n",
    "\n",
    "net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, lmbda = 0.1,monitor_evaluation_cost=True, monitor_evaluation_accuracy=True, monitor_training_cost=True, monitor_training_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works very well, even though it takes a while. But why ? Why are small weights better ? In what sense are they \"less complex\" ? \n",
    "\n",
    "Here's one way of seing this: when weights are small, changing a few inputs by a little bit at random won't have a huge effect on the network's output. This means that the network is robust, that it doen't contort its parameters to fit the data's noise. Note to self: use the word 'procrustean' somewhere here. \n",
    "\n",
    "Although Occam's razor is useful in many situations, it isn't a law of nature: sometimes the more complex answer can be correct (*eg* sometimes polynomal interpolation is preferable to linear regression). We apparently don't have a fully fledged theory of why regularization works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $L^1$ regularization\n",
    "\n",
    "We add a portion of the $L^1$ norm of the weight vector, instead of a portion of the square of the $L^2$ norm. The new cost function is:\n",
    "\n",
    "$$C = C_0 + \\frac \\lambda n \\sum_w |w|$$\n",
    "\n",
    "The weights are updated using the derivative of the previous formula (except when it isn't differentiable, in which case the unregularized rule is used):\n",
    "\n",
    "$$\\frac {\\partial C} {\\partial w} = \\frac {\\partial C_0} {\\partial w} + \\frac \\lambda n sgn(w)$$\n",
    "\n",
    "In both $L^1$ and $L^2$ regularization, every iteration reduces the value of the weights, but by a different amount. In $L^2$ regularization, this amount depends on the value of the weight being updated, whereas in $L^1$ regularization, this amount is always the same.\n",
    "\n",
    "Therefore $L^2$ has a uniformizing effect on the weights: bigger weights get \"taxed\" more than small weights. On the other hand, $L^1$ changes every weight by the same amount, and leads to a more irregular distribution of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout\n",
    "\n",
    "Dropout is a completely different technique, that doesn't involve tweaking the cost function. We use stochastic gradient descent, except for every mini-batch only part of the network is trained: some hidden neurons are \"ghosted\", propagation/backpropagation happens only on the remaining nodes. For every new mini-batch, a new set of hidden nodes is left out.\n",
    "\n",
    "Intuitively, this is very close to training different networks on the same data and choosing the output as the average of their outputs, or by making them \"vote\" for the right answer.\n",
    "\n",
    "Since at some point during the training, neurons will be learning while their neighours are \"ghosted\", the network is in a way trained to be robust with regards to changes in the output of nearby neurons. This is a good way of avoiding overfitting. Dropout works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Artificially expanding the training data\n",
    "\n",
    "If we add to the original training data pictures obtained by rotating (and/or translating, skewing) training pictures, we get more data (nearly) for free and make our network more robust with respect to these kinds of image transformations. \n",
    "Mike says: **\"The general principle is to expand the training data by applying operations that reflect real-world variation.\"**\n",
    "\n",
    "If we allow too large rotations of MNIST images, the network may end up producing nonsense. For instance, it may start confusing $6$ and $9$.\n",
    "\n",
    "\n",
    "### An aside on comparing algorithms\n",
    "\n",
    "An algorithm can be better than another on a certain dataset, and worse on others. So we should be wary of overly spectacular allegations regarding any given technique: for example, some algorithms are extremely efficient only to solve a very restricted set of problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Initializing the weight distribution \n",
    "Up to now, we've initialized or weights and biases at random, using a normal distribution:\n",
    "$$\\forall l, \\forall j, \\forall k, w^l_{jk} \\sim \\mathcal{N}(0, 1)$$\n",
    "$$\\forall l, \\forall j, b^l_{j} \\sim \\mathcal{N}(0, 1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the problem with that ? \n",
    "\n",
    "If the initial weights all follow this distribution, then the weighted input $z$ of any given neuron in the first layer is quite likely to be very big or very small, $|z| >> 1 $.\n",
    "\n",
    "For instance, if the input layer contains $1000$ neurons, for an input $x$ such that $x_i = 1$ for $500$ values of $i$ and $x_i = 0$ for the $500$ others, the weighted input $z$ will follow a centered normal distribution with a standard error close to $22.4$:\n",
    "\n",
    "$$z = \\sum^{500}_{i = 1} w_i + b_i \\sim \\mathcal{N}(0, 501 \\simeq 22.4^2)$$\n",
    "\n",
    "It is therefore quite probable that $|z| >> 1$, meaning that $\\sigma'(z) \\simeq 0$: as we saw previously in this chapter, this causes a learning slowdown. This kind of argument generalizes to weights in other layers: the idea is the same but the computations get more ugly.\n",
    "\n",
    "### What's a better choice ?\n",
    "\n",
    "Let $n_in$ be the number of input weights on a neuron. Then the weights toward that neuron should be initialized following a normal distribution of standard error $\\frac {1} {\\sqrt n_{in}}$.\n",
    "\n",
    "This way, with the same input as above, ie $n_{in} = 1000$:\n",
    "$$z = \\sum^{500}_{i = 1} w_i + b_i \\sim \\mathcal{N}(0, 1 + \\sum^{500}_{i = 1} \\frac {1}{n_{in}}) = \\mathcal{N}(0, 1 + \\frac 1 2) = \\mathcal{N}(0, \\frac 3 2)$$\n",
    "\n",
    "The standard deviation is $\\sqrt{\\frac 3 2 }$so that it is far less likely that $|z| >> 1$.\n",
    "\n",
    "Biases don't matter too much, so we'll keep initializing them according to $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this achieve in practice ?\n",
    "\n",
    "Our original approach to initialization can be compared to the one given above in `network2.py` file (or the first Jupyter notebook corresponding to this chapter) by using either the `default_weight_initializer` or the `large_weight_initializer` methods.\n",
    "\n",
    "Though the final accuracy is pretty similar, the network learns much faster when the weights are initialized with this newer method. There are other example of networks where both the speed and the final accuracy are improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Choosing hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
