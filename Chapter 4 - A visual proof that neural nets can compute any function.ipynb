{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "**Universality theorem** (qualitatively): for every function $f : \\mathbb{R}^m \\to \\mathbb{R }^n$, there exists a neural network with one hidden layer, that outputs a satisfactory approximation of $f$. The proof uses some functional analysis. Here Michael Nielsen gives a more intuitive explanation.\n",
    "\n",
    "Note that since most problems can be described as learning a function, this theorem is pretty amazing. However, it proves the existence of a neural network approximating a function, without giving a function to construction of such a network. It also doesn't guarantee that the network will be efficient.\n",
    "\n",
    "This notebook will be pretty short, as there's no code in this chapter, and not much to summarize: Nielsen's explanations are all that's needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Two caveats\n",
    "\n",
    "It isn't completely accurate to say: \"Neural networks can approximate any function\".\n",
    "## 1. We only get an approximation\n",
    "\n",
    "For every function $f : \\mathbb{R}^m \\to \\mathbb{R}^n$, for every accuracy $\\epsilon > 0$, there exists a neural network $N$, the output of which is given by the function $g$, such that:\n",
    "\n",
    "$$\\forall x \\in \\mathbb{R}^m, ||f(x) - g(x)|| \\leq \\epsilon $$\n",
    "\n",
    "For the network to be accurate, the number of hidden neurons has to grow.\n",
    "\n",
    "## 2. $f$ has to be continuous\n",
    "\n",
    "For the theorem to be valid, $f$ has to be continuous. However, even if $f$ is discontinous, more often than not a neural network will give a decent approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Plan of the explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, summarizing is not very useful here... all I can do is redirect to Nielsen's explanation, since it's more visual and well-crafted than anything I could write here.\n",
    "\n",
    "* when $f$ has one input, one output\n",
    " * how to (almost) build a step function with only $2$ hidden neurons\n",
    " * how to approximate $f$ by combining these step functions\n",
    "* what to do when $f$ has many input variables\n",
    "* how to deal with the fact that we're not building *actual* step functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Summary of an actual proof\n",
    "\n",
    "As Mike Nielsen explicitly mentions in the book, this chapter does not really proove the theorem: it's an intuitive and visual explanation of the mechanisms underlying an actual proof, such as [this one](http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf), which he links to in a footnote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally the set of weighted inputs to the output layer of neural networks with one hidden layer can be written as the set of $  \\sum_{j = 1}^{N} \\alpha_j \\sigma(w_j^T x + b_j)$, with the $w_j \\in \\mathbb{R}^n$ and the $\\alpha_j$ and $b_j$ in $\\mathbb{R}$.\n",
    "\n",
    "We are studying activation functions $\\sigma$ that are **sigmoidal**, i.e. \n",
    "$$lim_{t \\to + \\infty}\\sigma(t) = 1 $$\n",
    "$$lim_{t \\to - \\infty}\\sigma(t) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proof has two main steps. **First**, it is proven that continuous sigmoidal functions are **discriminatory**, meaning that for every Borel measure $\\mu$ on $[0,1]^n$: \n",
    "$$\\big[ \\forall w \\in \\mathbb{R}^n, \\forall b \\in \\mathbb{R}, \\int_{[0,1]^n} \\sigma(w^Tx + b) d\\mu(x) = 0 \\big] \\implies \\mu = 0$$\n",
    "\n",
    "Showing this involves looking at the limit of transforms of functions $\\sigma_{\\lambda, \\phi} : x \\mapsto \\sigma[\\lambda(w^Tx + b) + \\phi]$ when $\\lambda \\to \\infty$, and integrating using Lebesgue's dominated convergence theorem. This mirrors the approximation of step functions by heavily weighted sigmoids in Nielsen's explanation. \n",
    "\n",
    "**Second**, we show that if $\\sigma$ is continuous and discriminatory then the set $S$ of functions of the form $x \\mapsto  \\sum_{j = 1}^{N} \\alpha_j \\sigma(w_j^T x + b_j)$ is dense in $C^0 ([0,1]^n, \\mathbb{R})$. In particular, if $\\sigma$ is a bicontinuous bijection $\\mathbb{R} \\to ]0,1[$, then for every function $f: [0,1]^n \\to ]0,1[$, $\\sigma^{-1} \\circ f$ can be approximated with arbitrarily good precision, and so can $f$.\n",
    "\n",
    "Why is $S$ dense ? Suppose that $\\bar S \\subsetneq C^0 ([0,1]^n, \\mathbb{R})$. Then according to the **Hahn-Banach theorem**, there exists a continuous linear functional $F$ in the Hilbert space $C^0 ([0,1]^n,  \\mathbb{R})$ such that $F$ is nonzero but $F|_{\\bar S} = 0$. The **Riesz representation theorem** says that there exists a Borel measure $\\mu$ on $[0,1]^n$ such that:\n",
    "\n",
    "$$F : f \\in C^0 ([0,1]^n, \\mathbb{R}) \\mapsto \\int_{[0,1]^n} f(x)d\\mu(x)$$\n",
    "\n",
    "However, since $\\sigma$ is discriminatory, any Linear functional of this form that vanishes on $S$ has to be zero. We have reached a contradiction, which shows that necessarily $\\bar S = C^0 ([0,1]^n, \\mathbb{R})$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
